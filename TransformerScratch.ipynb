{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning\n",
    "# for some reason, need to run this twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfeloundou\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# ‚ö° ü§ù üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Text\n",
    "# from torchtext import data, datasets\n",
    "# import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "# from torchtext.datasets import IMDB\n",
    "from torchtext.datasets import IWSLT\n",
    "# from torchtext.datasets import WikiText2 #vocab size of 33,278\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DET'), ('is', 'AUX'), ('an', 'DET'), ('English', 'ADJ'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Voici', 'VERB'), ('une', 'DET'), ('phrase', 'NOUN'), ('en', 'ADP'), ('francais', 'NOUN'), ('.', 'PUNCT')]\n",
      "['Je', 'ne', 'suis', 'pas', 'une', 'malade', '.']\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download fr\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"This is an English sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "nlpf = spacy.load('fr')\n",
    "docu = nlpf(\"Voici une phrase en francais.\")\n",
    "print([(w.text, w.pos_) for w in docu])\n",
    "\n",
    "print([tok.text for tok in spacy_fr.tokenizer(\"Je ne suis pas une malade.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = {\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"model_dimension\" : 512,\n",
    "    \"num_layers\" : 6,\n",
    "    \"num_heads\" : 8,\n",
    "#     \"batch_size\" : 4096, # batch size from the paper\n",
    "    \"batch_size\" : 8,\n",
    "    \"dropout\" : 0.1,\n",
    "    \"label_smoothing\" : 0.1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x7f26b7c7ad00>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BLANK_TOKEN = \"<blank>\"\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_fr = spacy.load('fr')\n",
    "\n",
    "def tokenize_french(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_french, pad_token=BLANK_TOKEN)\n",
    "TGT = Field(tokenize=tokenize_english, init_token = BOS_TOKEN, eos_token = EOS_TOKEN, pad_token=BLANK_TOKEN)\n",
    "\n",
    "MAX_LEN = 100\n",
    "train, val, test = IWSLT.splits(\n",
    "    exts=('.fr', '.en'), fields=(SRC, TGT), \n",
    "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 2\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atoi  -> ASCII to integer.\n",
    "# atol  -> ASCII to long.\n",
    "# atof  -> ASCII to floating.\n",
    "# stoi  -> string to integer.\n",
    "# stol  -> string to long.\n",
    "# stoll -> string to long long.\n",
    "# stof  -> string to float. \n",
    "# stod  -> string to double.\n",
    "# stold -> string to long double.\n",
    "# itos  -> integer to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33130\n",
      "Ou\n",
      "dog\n",
      "\n",
      "\n",
      "27375\n",
      "sitting\n",
      "canine\n"
     ]
    }
   ],
   "source": [
    "print(SRC.vocab.stoi['dog'])\n",
    "print(SRC.vocab.itos[666])\n",
    "print(SRC.vocab.itos[33130])\n",
    "\n",
    "print('\\n')\n",
    "print(TGT.vocab.stoi['canine'])\n",
    "print(TGT.vocab.itos[666])\n",
    "print(TGT.vocab.itos[27375])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a batch of 4 sentences\n",
    "train_iter = BucketIterator(train, batch_size=4, sort_key=lambda x: len(x.trg), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  102,    26,    90,    48],\n",
      "        [    2,    65,    10,    81],\n",
      "        [16010,    28,   200,    87],\n",
      "        [    7,    32,    35,  5896],\n",
      "        [   31,    78,     5,    89],\n",
      "        [13999,   769,  1858,   235],\n",
      "        [   29,    25,     2, 45549],\n",
      "        [ 3333,     2,   171,     3],\n",
      "        [    9,    33,    14,  1178],\n",
      "        [    5,    60,   142,   396],\n",
      "        [  128,    24,     4,     4],\n",
      "        [   24,    11, 15765,    13],\n",
      "        [   59,  1229,     2,  3090],\n",
      "        [   14,    72,    10,     2],\n",
      "        [13906,   213,  1905,     4],\n",
      "        [    6,     3,    20,  1333],\n",
      "        [   14,     1,   199,     4],\n",
      "        [ 9806,     1,     9,   724],\n",
      "        [    3,     1,   304,     2],\n",
      "        [    1,     1,    49,     4],\n",
      "        [    1,     1,  1589,  2011],\n",
      "        [    1,     1,     2,     4],\n",
      "        [    1,     1,   151,  1262],\n",
      "        [    1,     1,   171,    50],\n",
      "        [    1,     1,   123,   238],\n",
      "        [    1,     1,  7977,   263],\n",
      "        [    1,     1,    10,    15],\n",
      "        [    1,     1,   688,  1105],\n",
      "        [    1,     1,     3,     3]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = next(iter(train_iter))\n",
    "'''In each batch, the sentences have been transposed so they are descending vertically \n",
    "(important: we will need to transpose these again to work with the transformer). Each index represents a token (word), \n",
    "and each column represents a sentence. We have 10 columns, as 10 was the batch_size we specified.'''\n",
    "\n",
    "print(batch.src) # source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,     2],\n",
      "        [   32,    19,    90,    32],\n",
      "        [12983,    62,    35,    12],\n",
      "        [   14,    76,    79,    22],\n",
      "        [  198,   104,    27,   236],\n",
      "        [   39,    40,    10,    39],\n",
      "        [  409,   179,  1797, 15149],\n",
      "        [13523,    67,     4,   237],\n",
      "        [    9,     4,   190,     4],\n",
      "        [ 7301,    16,   102,    51],\n",
      "        [  121,    22,     8,    34],\n",
      "        [ 3036,     8, 19484,    22],\n",
      "        [    5,   202,     4,     6],\n",
      "        [    3,     7,     6,  2870],\n",
      "        [    1,  1147,   648,     4],\n",
      "        [    1,    67,   187,     6],\n",
      "        [    1,     5,     7,  3790],\n",
      "        [    1,     3,   636,     4],\n",
      "        [    1,     1,     4,     6],\n",
      "        [    1,     1,    55,   822],\n",
      "        [    1,     1,   190,     4],\n",
      "        [    1,     1,  6325,     6],\n",
      "        [    1,     1,     5,  1406],\n",
      "        [    1,     1,     3,  2561],\n",
      "        [    1,     1,     1,    11],\n",
      "        [    1,     1,     1,    60],\n",
      "        [    1,     1,     1,   243],\n",
      "        [    1,     1,     1,  1353],\n",
      "        [    1,     1,     1,    13],\n",
      "        [    1,     1,     1,   638],\n",
      "        [    1,     1,     1,    31],\n",
      "        [    1,     1,     1,     3]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.trg) # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.English))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.French) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = './transformer_fr_en.pth'\n",
    "\n",
    "DATA_PATH=Path('./data/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BLANK_TOKEN = \"<blank>\"\n",
    "MAX_LEN = 100  # filter out examples that have more than MAX_LEN tokens\n",
    "MIN_FREQ = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache(cache_path, dataset):\n",
    "    with open(cache_path, 'w', encoding='utf-8') as cache_file:\n",
    "        # Interleave source and target tokenized examples, source is on even lines, target is on odd lines\n",
    "        for ex in dataset.examples:\n",
    "            cache_file.write(' '.join(ex.src) + '\\n')\n",
    "            cache_file.write(' '.join(ex.trg) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_language_datasets(data_dir, max_len = MAX_LEN, min_freq = MIN_FREQ):\n",
    "\n",
    "    spacy_en = spacy.load('en')\n",
    "    spacy_fr = spacy.load('fr')\n",
    "\n",
    "    def tokenize_fr(text):\n",
    "        return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    \n",
    "    # Tokenize for the source and target\n",
    "    src_tokenizer = tokenize_en\n",
    "    trg_tokenizer = tokenize_fr\n",
    "    \n",
    "    src_field_processor = Field(tokenize=src_tokenizer, pad_token=PAD_TOKEN, batch_first=True) # Whether to produce tensors with the batch dimension first. Default: False.\n",
    "    trg_field_processor = Field(tokenize=trg_tokenizer, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
    "\n",
    "    fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
    "\n",
    "    # Only call once the splits function it is super slow as it constantly has to redo the tokenization\n",
    "    prefix = 'en_fr_iwslt'\n",
    "    \n",
    "    train_cache_path = os.path.join(data_dir, f'{prefix}_train_cache.csv')\n",
    "    val_cache_path = os.path.join(data_dir, f'{prefix}_val_cache.csv')\n",
    "    test_cache_path = os.path.join(data_dir, f'{prefix}_test_cache.csv')\n",
    "\n",
    "    # This simple caching mechanism gave me ~30x speedup on my machine! From ~70s -> ~2.5s!\n",
    "    ts = time.time()\n",
    " \n",
    "    src_ext = '.en' \n",
    "    trg_ext = '.fr' \n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = IWSLT.splits(\n",
    "        exts=(src_ext, trg_ext),\n",
    "        fields=fields,\n",
    "        root=data_dir,\n",
    "        filter_pred=lambda x: len(x.src) <= max_len and len(x.trg) <= max_len\n",
    "    )\n",
    "    \n",
    "\n",
    "    save_cache(train_cache_path, train_dataset)\n",
    "    save_cache(val_cache_path, val_dataset)\n",
    "    save_cache(test_cache_path, test_dataset)\n",
    "\n",
    "#     print(f'Time it took to prepare the data: {time.time() - ts:3f} seconds.')\n",
    "    print('It took {} seconds to prepare the data.'.format(time.time()-ts))\n",
    "\n",
    "    # __getattr__ implementation in the base Dataset class enables us to call .src on Dataset objects even though\n",
    "    # we only have a list of examples in the Dataset object and the example itself had .src attribute.\n",
    "    # Implementation will yield examples and call .src/.trg attributes on them (and those contain tokenized lists)\n",
    "    src_field_processor.build_vocab(train_dataset.src, min_freq=min_freq)\n",
    "    trg_field_processor.build_vocab(train_dataset.trg, min_freq=min_freq)\n",
    "\n",
    "    return train_dataset, val_dataset, src_field_processor, trg_field_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to get efficient batching\n",
    "\n",
    "'''While Torchtext is brilliant, it‚Äôs sort_key-based batching leaves \n",
    "a little to be desired. Often the sentences are of different lengths, \n",
    "and you end up feeding a lot of padding into your network \n",
    "(as you can see with all the 1s in the last figure).\n",
    "\n",
    "Additionally, if your RAM can process say 1500 tokens each iteration, \n",
    "and your batch_size is 20, then only when you have batches of length 75 \n",
    "utilising all the memory. \n",
    "\n",
    "An efficient batching mechanism would change the batch size \n",
    "depending on the sequence length to make sure around 1500 tokens were being processed each iteration.'''\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))  #can change this .src to a more generic extension, #TODO: Review\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to get masks and count the tokens in source and target sentences. Note that \n",
    "\n",
    "def masks_tokens(src_token_ids_batch, trg_token_ids_batch, pad_token_id, device):\n",
    "    \n",
    "    def masks_tokens_src(src_token_ids_batch, pad_token_id):\n",
    "        batch_size = src_token_ids_batch.shape[0]\n",
    "\n",
    "        # src_mask shape = (B, 1, 1, S) #TODO: check this\n",
    "        # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
    "        src_mask = (src_token_ids_batch != pad_token_id).view(batch_size, 1, 1, -1)\n",
    "        num_src_tokens = torch.sum(src_mask.long())\n",
    "\n",
    "        return src_mask, num_src_tokens\n",
    "\n",
    "\n",
    "\n",
    "    def masks_tokens_trg(trg_token_ids_batch, pad_token_id):\n",
    "        batch_size = trg_token_ids_batch.shape[0]\n",
    "        device = trg_token_ids_batch.device\n",
    "\n",
    "        # Same as src_mask but we additionally want to mask future tokens (want to predict)\n",
    "        # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "        sequence_length = trg_token_ids_batch.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "        trg_padding_mask = (trg_token_ids_batch != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
    "        trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "        # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "        trg_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "        num_trg_tokens = torch.sum(trg_padding_mask.long())\n",
    "\n",
    "        return trg_mask, num_trg_tokens\n",
    "\n",
    "    src_mask, num_src_tokens = masks_tokens_src(src_token_ids_batch, pad_token_id)\n",
    "    trg_mask, num_trg_tokens = masks_tokens_trg(trg_token_ids_batch, pad_token_id)\n",
    "\n",
    "    return src_mask, trg_mask, num_src_tokens, num_trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of our samples\n",
    "def custom_dataloader(data_dir, batch_size, device):\n",
    "    train_dataset, val_dataset, src_field_processor, trg_field_processor = get_language_datasets(data_dir=DATA_PATH)\n",
    "    \n",
    "    '''batch_size_fn: \n",
    "    Function of three arguments (new example to add, current count of examples in the batch, and current effective batch size)\n",
    "            that returns the new effective batch size resulting from adding\n",
    "            that example to a batch. This is useful for dynamic batching, where\n",
    "            this function would add to the current effective batch size the\n",
    "            number of tokens in the new example.'''\n",
    "\n",
    "    # using default sorting function which\n",
    "    train_token_ids_loader, val_token_ids_loader = BucketIterator.splits(\n",
    "     datasets=(train_dataset, val_dataset), batch_size=batch_size,\n",
    "     device=device,\n",
    "     sort_within_batch=True,\n",
    "     batch_size_fn=batch_size_fn\n",
    "    )\n",
    "\n",
    "    return train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 68.31505393981934 to prepare the data.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor = custom_dataloader(data_dir=DATA_PATH, \n",
    "                                                                                                           batch_size=8, device=device)\n",
    "\n",
    "# These are generator objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The source vocabulary length: 38627.\n",
      "The target vocabulary length: 47668.\n"
     ]
    }
   ],
   "source": [
    "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "print(pad_token_id)\n",
    "\n",
    "for batch in train_token_ids_loader:\n",
    "    # Visually inspect that masks make sense\n",
    "    src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = masks_tokens(batch.src, batch.trg, pad_token_id, device)\n",
    "    break\n",
    "\n",
    "# Check vocab size\n",
    "print('The source vocabulary length: {}.'.format(len(src_field_processor.vocab)))\n",
    "print('The target vocabulary length: {}.'.format(len(trg_field_processor.vocab)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Source text:\tYou are going to fake it . \n",
      "Target text:\t<s> Vous allez faire semblant . </s> \n",
      "----------\n",
      "Source text:\tSo the thing is , this kind of idea of Chinese - American food does n't exist only in America . \n",
      "Target text:\t<s> Mais en fait , ce genre d' id√©e qu' on se fait de la cuisine sino-am√©ricaine n' existe pas qu' en Am√©rique . </s> \n",
      "----------\n",
      "Source text:\tHe looks like this . \n",
      "Target text:\t<s> Il ressemble √† ceci . </s> \n",
      "----------\n",
      "Source text:\tSo again , people discover their creative agency in this way . \n",
      "Target text:\t<s> Une fois de plus , les gens d√©couvrent leur r√¥le cr√©atif de cette mani√®re . </s> \n",
      "----------\n",
      "Source text:\tIt was like breathing . \n",
      "Target text:\t<s> C' √©tait comme respirer . </s> \n"
     ]
    }
   ],
   "source": [
    "# helper function to inspect the text\n",
    "def sample_text_from_loader(src_field_processor, trg_field_processor, token_ids_loader, \n",
    "                            num_samples=2, sample_src=True, sample_trg=True, show_padded=False):\n",
    "    \n",
    "    assert sample_src or sample_trg, f'Either src or trg or both must be enabled.'\n",
    "\n",
    "    for b_idx, token_ids_batch in enumerate(token_ids_loader):\n",
    "        if b_idx == num_samples:  # Number of sentence samples to print\n",
    "            break\n",
    "\n",
    "        print('-' * 10)\n",
    "        if sample_src:\n",
    "            print(\"Source text:\", end=\"\\t\")\n",
    "            for token_id in token_ids_batch.src[0]:  # print only the first example from the batch\n",
    "                src_token = src_field_processor.vocab.itos[token_id]\n",
    "\n",
    "                if src_token == PAD_TOKEN and not show_padded:\n",
    "                    continue\n",
    "\n",
    "                print(src_token, end=\" \")\n",
    "            print()\n",
    "\n",
    "        if sample_trg:\n",
    "            print(\"Target text:\", end=\"\\t\")\n",
    "            for token_id in token_ids_batch.trg[0]:\n",
    "                trg_token = trg_field_processor.vocab.itos[token_id]\n",
    "\n",
    "                if trg_token == PAD_TOKEN and not show_padded:\n",
    "                    continue\n",
    "\n",
    "                print(trg_token, end=\" \")\n",
    "            print()\n",
    "\n",
    "sample_text_from_loader(src_field_processor, trg_field_processor, train_token_ids_loader, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jachiam\n",
    "\n",
    "\n",
    "B = 5\n",
    "L = 32\n",
    "d = 16\n",
    "w = 8\n",
    "\n",
    "x = torch.as_tensor(np.random.rand(B,L,d), dtype=torch.float32)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in = 16, d_proj = 8, n_heads = 2, causal = True):  #ask josh why causal here  \n",
    "        #a causal convolution has a window that overlaps only the current and previous timesteps\n",
    "        #Causal convolutions are necessary because it would be cheating if the CNN was able to \n",
    "        #‚Äúsee‚Äù information from the future timesteps that it is trying to predict.\n",
    "        super().__init__()\n",
    "        self.d_proj = d_proj\n",
    "        self.n_heads = n_heads\n",
    "        self.qkv_lin = nn.Linear(d_in, 3 * d_proj * n_heads, bias=False)\n",
    "        self.causal = True\n",
    "\n",
    "    def _mask(self, qk):\n",
    "        mask = 1e9 * (torch.tril(torch.ones_like(qk)) - 1)\n",
    "        return torch.tril(qk) + mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv_lin = self.qkv_lin(x).reshape(*x.shape[:2], self.n_heads, self.d_proj * 3) # B x L x n x 3d\n",
    "        qkv_lin = qkv_lin.permute(0,2,1,3)                  # B x n x L x 3d\n",
    "        q, k, v = torch.split(qkv_lin, self.d_proj, dim=-1) # B x n x L x d\n",
    "        q_pre = q.reshape(-1, *q.shape[2:])                 # Bn x L x d\n",
    "        k_pre = k.reshape(-1, *k.shape[2:]).permute(0,2,1)  # Bn x d x L\n",
    "        v_pre = v.reshape(-1, *v.shape[2:])                 # Bn x L x d\n",
    "        qk = torch.bmm(q_pre,k_pre)                         # Bn x L x L\n",
    "        qk_ = self._mask(qk) if self.causal else qk\n",
    "        softmax_qk = torch.softmax(qk_, axis=-1)\n",
    "        \n",
    "        y = torch.bmm(softmax_qk, v_pre)                    # Bn x L x d\n",
    "        y = y.reshape(x.shape[0], self.n_heads, x.shape[1], self.d_proj)  # B x n x L x d\n",
    "        y = y.permute(0,2,1,3)\n",
    "        y = y.reshape(*x.shape[:2],-1)\n",
    "        return y, softmax_qk.reshape(x.shape[0], self.n_heads, x.shape[1], x.shape[1])\n",
    "        #q, k, v = torch.split(qkv_lin, self.d_proj, dim=2)\n",
    "        #return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7247, -1.0848, -1.0903],\n",
      "         [-0.0517,  0.1599, -0.1975],\n",
      "         [ 0.1476, -0.1203, -1.3196]],\n",
      "\n",
      "        [[ 0.1476, -0.1203, -1.3196],\n",
      "         [ 0.1189, -1.7153, -1.1312],\n",
      "         [-0.0517,  0.1599, -0.1975]],\n",
      "\n",
      "        [[-0.0517,  0.1599, -0.1975],\n",
      "         [ 0.1189, -1.7153, -1.1312],\n",
      "         [-0.7247, -1.0848, -1.0903]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[ 0.2512, -1.5186,  0.7122],\n",
      "         [-1.5975, -1.2887, -0.9902]],\n",
      "\n",
      "        [[ 2.0694,  0.3160, -0.1283],\n",
      "         [-0.3061, -0.9990, -0.3430]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3) # an Embedding module containing 10 tensors of size 3\n",
    "embedding\n",
    "\n",
    "input = torch.LongTensor([[1,2,4],[4,3,2], [2,3,1]])\n",
    "input2 = torch.LongTensor([[0,8], [7,6]])\n",
    "print(embedding(input))\n",
    "print(embedding(input2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input : Embeddings + Positional Encodings\n",
    "src_vocab_size = len(src_field_processor.vocab)\n",
    "trg_vocab_size = len(trg_field_processor.vocab)\n",
    "# Embeddings\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "#         self.embeddings_lookup = nn.Embedding(length,  embedding_dim = 512)\n",
    "        self.source_embedding = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim = 512)\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=trg_vocab_size, embedding_dim = 512)\n",
    "        \n",
    "        \n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "src_vocab_size = 20\n",
    "trg_vocab_size = 20\n",
    "src_token_ids_batch = torch.randint(1, 10, size=(3, 2))\n",
    "trg_token_ids_batch = torch.randint(1, 10, size=(3, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout_rate = 0.1\n",
    "\n",
    "# Multiheaded attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Implementation (In the AIAYN paper, we will stack 6 of these, may experiment with more)\n",
    "# In each encoder layer, there is an attention mechanism and there a FFNN\n",
    "\n",
    "\n",
    "\n",
    "# 1. Multihead Attention = Z\n",
    "\n",
    "# 2. LayerNorm(Z + Drop(Z)) = Y\n",
    "\n",
    "# 3. LayerNorm(Y + Drop(Fat-Relu(Y)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clones(module, num_copies):\n",
    "    # Create layer clones that can be adjusted separately, rather than referentially\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_copies)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
