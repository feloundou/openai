{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "TransformerScratch.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rX9FfGJrhvH"
      },
      "source": [
        "%%capture\n",
        "! pip install -qqq wandb pytorch-lightning\n",
        "! python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ZBpEVB5DCc"
      },
      "source": [
        "import pytorch_lightning\n",
        "# for some reason, need to run this twice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxbmZIB2rhvH"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import math, copy, time\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# import pytorch_lightning as pl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-g_k6v4_1m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlOGZApQrhvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d8555259-1041-4b22-ce0e-dd09256e4f24"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "pl.seed_everything(hash(\"set random seeds\") % 2**32 - 1)\n",
        "\n",
        "\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GJqoWxGrhvH"
      },
      "source": [
        "# Test Text\n",
        "# from torchtext import data, datasets\n",
        "# import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "# from torchtext.datasets import IMDB\n",
        "from torchtext.datasets import IWSLT\n",
        "# from torchtext.datasets import WikiText2 #vocab size of 33,278\n",
        "import spacy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrPTb8NBrhvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884c0ef5-1fc6-44ac-c9bf-8873fadc0874"
      },
      "source": [
        "# ! python -m spacy download fr\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(\"This is an English sentence.\")\n",
        "print([(w.text, w.pos_) for w in doc])\n",
        "\n",
        "nlpf = spacy.load('fr')\n",
        "docu = nlpf(\"Voici une phrase en francais.\")\n",
        "print([(w.text, w.pos_) for w in docu])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('This', 'DET'), ('is', 'AUX'), ('an', 'DET'), ('English', 'ADJ'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n",
            "[('Voici', 'VERB'), ('une', 'DET'), ('phrase', 'NOUN'), ('en', 'ADP'), ('francais', 'NOUN'), ('.', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnXwhg3brhvH"
      },
      "source": [
        "\n",
        "PATH = './transformer_fr_en.pth'\n",
        "\n",
        "DATA_PATH=Path('./data/')\n",
        "DATA_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "BOS_TOKEN = '<s>'\n",
        "EOS_TOKEN = '</s>'\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "BLANK_TOKEN = \"<blank>\"\n",
        "MAX_LEN = 100  # filter out examples that have more than MAX_LEN tokens\n",
        "MIN_FREQ = 2\n",
        "\n",
        "args = {\n",
        "    \"full_data_dir\": DATA_PATH,\n",
        "    \"model_dimension\" : 512,\n",
        "    \"num_layers\" : 6,\n",
        "    \"num_heads\" : 8,\n",
        "#     \"batch_size\" : 4096, # batch size from the paper\n",
        "    \"batch_size\" : 8,\n",
        "    \"dropout\" : 0.1,\n",
        "    \"label_smoothing\" : 0.1\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQKCYV72zGOK"
      },
      "source": [
        "def greedy_decode_sequence(model, X, start_idx_Y, pad_idx_X, pad_idx_Y, max_len):\n",
        "    '''\n",
        "    Greedy Decoding\n",
        "    X : a sequence of input symbols. shape(batch_size=b, inp_len)\n",
        "    start_idx_Y: int.\n",
        "    pad_idx_X: int.\n",
        "    pad_idx_Y: int.\n",
        "    max_len: int. Maximum context length for generation. Include <SOS> and <EOS>\n",
        "    '''\n",
        "\n",
        "    b, inp_len = X.shape\n",
        "    inp_pads = (X == pad_idx_X).int()\n",
        "\n",
        "    # encode inputs\n",
        "    # shape (b, inp_len, d_model)\n",
        "    encoded_memory = model.encode(X, inp_pads)\n",
        " \n",
        "    # shape (b, max_len) e.g. (b, 10)\n",
        "    Y = torch.ones(b, max_len).type_as(X) * pad_idx_Y\n",
        "    Y[:,0] = start_idx_Y\n",
        "    # shape (b, max_len)\n",
        "    out_pads = (Y == pad_idx_Y).int()\n",
        "  \n",
        "    # generate one token at a time\n",
        "    for t in range(1, max_len): # (1 to 9)\n",
        "\n",
        "        # shape (b, t, d_model)\n",
        "        decoder_output = model.decode(encoded_memory, Y[:, :t], inp_pads, out_pads[:, :t])\n",
        "\n",
        "        # shape (b, t, V)\n",
        "        decoded_logits = model.classifier(decoder_output)\n",
        "        # decoded_probs = torch.softmax(decoded_logits, dim=-1)\n",
        "    \n",
        "        # shape (b,), (b,)\n",
        "        max_prob, max_idx = torch.max(decoded_logits[:, -1, :], dim=-1)\n",
        "    \n",
        "        # update Y, out_pads for next timestep\n",
        "        Y[:, t] = max_idx\n",
        "        out_pads[:, t] = (max_idx == pad_idx_Y).int()\n",
        "\n",
        "    # shape (b, max_len)\n",
        "    # (1 to 9)\n",
        "    return Y[:, 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyb6q7DQrhvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0658943c-b618-49a1-99d4-e105471ece9d"
      },
      "source": [
        "\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "spacy_fr = spacy.load('fr')\n",
        "\n",
        "def tokenize_french(text):\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "SRC = Field(tokenize=tokenize_french, pad_token=BLANK_TOKEN)\n",
        "TGT = Field(tokenize=tokenize_english, init_token = BOS_TOKEN, eos_token = EOS_TOKEN, pad_token=BLANK_TOKEN)\n",
        "\n",
        "MAX_LEN = 100\n",
        "train, val, test = IWSLT.splits(\n",
        "    exts=('.fr', '.en'), fields=(SRC, TGT), \n",
        "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
        "MIN_FREQ = 2\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
        "\n",
        "SRC\n",
        "print([tok.text for tok in spacy_fr.tokenizer(\"Je ne suis pas une malade.\")])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading fr-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fr-en.tgz: 100%|██████████| 25.7M/25.7M [00:07<00:00, 3.52MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/fr-en/IWSLT16.TED.tst2013.fr-en.en.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2010.fr-en.fr.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2011.fr-en.fr.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2014.fr-en.en.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.dev2010.fr-en.fr.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2013.fr-en.fr.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2014.fr-en.fr.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2011.fr-en.en.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2012.fr-en.fr.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2012.fr-en.en.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.dev2010.fr-en.en.xml\n",
            ".data/iwslt/fr-en/IWSLT16.TED.tst2010.fr-en.en.xml\n",
            ".data/iwslt/fr-en/train.tags.fr-en.fr\n",
            ".data/iwslt/fr-en/train.tags.fr-en.en\n",
            "['Je', 'ne', 'suis', 'pas', 'une', 'malade', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIzb7-Gn9a6G",
        "outputId": "87aeae32-b915-46ee-88f5-3864ac2bf5a1"
      },
      "source": [
        "type(train.src)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G3X84WNrhvJ"
      },
      "source": [
        "# atoi  -> ASCII to integer.\n",
        "# atol  -> ASCII to long.\n",
        "# atof  -> ASCII to floating.\n",
        "# stoi  -> string to integer.\n",
        "# stol  -> string to long.\n",
        "# stoll -> string to long long.\n",
        "# stof  -> string to float. \n",
        "# stod  -> string to double.\n",
        "# stold -> string to long double.\n",
        "# itos  -> integer to string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYtb1HzMrhvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd11827-6650-4842-aebd-d90662412947"
      },
      "source": [
        "print(SRC.vocab.stoi['dog'])\n",
        "print(SRC.vocab.itos[666])\n",
        "print(SRC.vocab.itos[33130])\n",
        "\n",
        "print('\\n')\n",
        "print(TGT.vocab.stoi['canine'])\n",
        "print(TGT.vocab.itos[666])\n",
        "print(TGT.vocab.itos[27375])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33083\n",
            "5\n",
            "déambuler\n",
            "\n",
            "\n",
            "27369\n",
            "sitting\n",
            "capitalization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccxnJ9uHrhvJ"
      },
      "source": [
        "# Let's look at a batch of 4 sentences\n",
        "train_iter = BucketIterator(train, batch_size=4, sort_key=lambda x: len(x.trg), shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIwMljeJrhvJ"
      },
      "source": [
        "\n",
        "train_iter = BucketIterator(train, batch_size=4, sort_key=lambda x: len(x.trg), shuffle=True)\n",
        "\n",
        "batch = next(iter(train_iter))\n",
        "'''In each batch, the sentences have been transposed so they are descending vertically \n",
        "(important: we will need to transpose these again to work with the transformer). Each index represents a token (word), \n",
        "and each column represents a sentence. We have 10 columns, as 10 was the batch_size we specified.'''\n",
        "\n",
        "print(batch.src) # source\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heed05E8rhvJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j62LTYZirhvJ"
      },
      "source": [
        "print(batch.trg) # target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSP_yzogrhvJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dyzh7cprhvJ"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If7WaIfArhvJ"
      },
      "source": [
        "def save_cache(cache_path, dataset):\n",
        "    with open(cache_path, 'w', encoding='utf-8') as cache_file:\n",
        "        # Interleave source and target tokenized examples, source is on even lines, target is on odd lines\n",
        "        for ex in dataset.examples:\n",
        "            cache_file.write(' '.join(ex.src) + '\\n')\n",
        "            cache_file.write(' '.join(ex.trg) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L8k87ewrhvJ"
      },
      "source": [
        "\n",
        "def get_language_datasets(data_dir, max_len = MAX_LEN, min_freq = MIN_FREQ):\n",
        "\n",
        "    spacy_en = spacy.load('en')\n",
        "    spacy_fr = spacy.load('fr')\n",
        "\n",
        "    def tokenize_fr(text):\n",
        "        return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    \n",
        "    # Tokenize for the source and target\n",
        "    src_tokenizer = tokenize_en\n",
        "    trg_tokenizer = tokenize_fr\n",
        "    \n",
        "    src_field_processor = Field(tokenize=src_tokenizer, pad_token=PAD_TOKEN, batch_first=True) # Whether to produce tensors with the batch dimension first. Default: False.\n",
        "    trg_field_processor = Field(tokenize=trg_tokenizer, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
        "\n",
        "    fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
        "\n",
        "    # Only call once the splits function it is super slow as it constantly has to redo the tokenization\n",
        "    prefix = 'en_fr_iwslt'\n",
        "    \n",
        "    train_cache_path = os.path.join(data_dir, f'{prefix}_train_cache.csv')\n",
        "    val_cache_path = os.path.join(data_dir, f'{prefix}_val_cache.csv')\n",
        "    test_cache_path = os.path.join(data_dir, f'{prefix}_test_cache.csv')\n",
        "\n",
        "    # This simple caching mechanism gave me ~30x speedup on my machine! From ~70s -> ~2.5s!\n",
        "    ts = time.time()\n",
        " \n",
        "    src_ext = '.en' \n",
        "    trg_ext = '.fr' \n",
        "    \n",
        "    train_dataset, val_dataset, test_dataset = IWSLT.splits(\n",
        "        exts=(src_ext, trg_ext),\n",
        "        fields=fields,\n",
        "        root=data_dir,\n",
        "        filter_pred=lambda x: len(x.src) <= max_len and len(x.trg) <= max_len\n",
        "    )\n",
        "    \n",
        "\n",
        "    save_cache(train_cache_path, train_dataset)\n",
        "    save_cache(val_cache_path, val_dataset)\n",
        "    save_cache(test_cache_path, test_dataset)\n",
        "\n",
        "#     print(f'Time it took to prepare the data: {time.time() - ts:3f} seconds.')\n",
        "    print('It took {} seconds to prepare the data.'.format(time.time()-ts))\n",
        "\n",
        "    # __getattr__ implementation in the base Dataset class enables us to call .src on Dataset objects even though\n",
        "    # we only have a list of examples in the Dataset object and the example itself had .src attribute.\n",
        "    # Implementation will yield examples and call .src/.trg attributes on them (and those contain tokenized lists)\n",
        "    src_field_processor.build_vocab(train_dataset.src, min_freq=min_freq)\n",
        "    trg_field_processor.build_vocab(train_dataset.trg, min_freq=min_freq)\n",
        "\n",
        "    return train_dataset, val_dataset, src_field_processor, trg_field_processor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "eNf5NREoJfgo",
        "outputId": "e07c6557-d4a3-4fa6-bd68-faec9c3c400e"
      },
      "source": [
        "print(train_dataset[0])\n",
        "\n",
        "\n",
        "class ENFRDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, data_dir='./', batch_size=128):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "    def tokenize_fr(text):\n",
        "        return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download data, train then test\n",
        "\n",
        "        spacy_en = spacy.load('en')\n",
        "        spacy_fr = spacy.load('fr')\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "      \n",
        "        # Tokenize for the source and target\n",
        "        src_tokenizer = tokenize_en\n",
        "        trg_tokenizer = tokenize_fr\n",
        "    \n",
        "        src_field_processor = Field(tokenize=src_tokenizer, pad_token=PAD_TOKEN, batch_first=True) # Whether to produce tensors with the batch dimension first. Default: False.\n",
        "        trg_field_processor = Field(tokenize=trg_tokenizer, init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN, batch_first=True)\n",
        "\n",
        "        fields = [('src', src_field_processor), ('trg', trg_field_processor)]\n",
        "\n",
        "\n",
        "        src_ext = '.en' \n",
        "        trg_ext = '.fr' \n",
        "        \n",
        "        # we set up only relevant datasets when stage is specified\n",
        "        if stage == 'fit' or stage is None:\n",
        "           self.train_dataset, self.val_dataset, self.test_dataset = IWSLT.splits(\n",
        "            exts=(src_ext, trg_ext),\n",
        "            fields=fields,\n",
        "            root=self.data_dir,\n",
        "            filter_pred=lambda x: len(x.src) <= max_len and len(x.trg) <= max_len\n",
        "        )\n",
        "           src_field_processor.build_vocab(self.train_dataset.src, min_freq=min_freq)\n",
        "           trg_field_processor.build_vocab(self.train_dataset.trg, min_freq=min_freq)\n",
        "\n",
        "            # mnist = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            # self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
        "        if stage == 'test' or stage is None:\n",
        "           self.train_dataset, self.val_dataset, self.test_dataset = IWSLT.splits(\n",
        "            exts=(src_ext, trg_ext),\n",
        "            fields=fields,\n",
        "            root=self.data_dir,\n",
        "            filter_pred=lambda x: len(x.src) <= max_len and len(x.trg) <= max_len\n",
        "        )\n",
        "           src_field_processor.build_vocab(self.train_dataset.src, min_freq=min_freq)\n",
        "           trg_field_processor.build_vocab(self.train_dataset.trg, min_freq=min_freq)\n",
        "\n",
        "            # self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    # we define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        enfr_train = DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        train_iter = BucketIterator(self.train_dataset, batch_size=self.batch_size, sort_key=lambda x: len(x.trg), shuffle=True)\n",
        "        train_batch = next(iter(train_iter))\n",
        "        # print(train_batch.src) \n",
        "        return train_batch\n",
        "\n",
        "        # return enfr_train\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        enfr_val = DataLoader(self.val_dataset, batch_size=10 * self.batch_size)\n",
        "\n",
        "        val_iter = BucketIterator(self.val_dataset, batch_size=self.batch_size, sort_key=lambda x: len(x.trg), shuffle=True)\n",
        "        val_batch = next(iter(val_iter))\n",
        "        return val_batch\n",
        "        # return enfr_val\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        enfr_test = DataLoader(self.test_dataset, batch_size=10 * self.batch_size)\n",
        "        test_iter = BucketIterator(self.train_dataset, batch_size=self.batch_size, sort_key=lambda x: len(x.trg), shuffle=True)\n",
        "        test_batch = next(iter(test_iter))\n",
        "\n",
        "        return test_batch\n",
        "        # return enfr_test\n",
        "\n",
        "# setup data\n",
        "enfr = ENFRDataModule(data_dir = DATA_PATH)\n",
        "enfr.setup()\n",
        "\n",
        "# grab samples to log predictions on\n",
        "samples = next(iter(enfr.val_dataloader()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-62d45afe3c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0081lOVrhvJ"
      },
      "source": [
        "# # How to get efficient batching\n",
        "\n",
        "# '''While Torchtext is brilliant, it’s sort_key-based batching leaves \n",
        "# a little to be desired. Often the sentences are of different lengths, \n",
        "# and you end up feeding a lot of padding into your network \n",
        "# (as you can see with all the 1s in the last figure).\n",
        "\n",
        "# Additionally, if your RAM can process say 1500 tokens each iteration, \n",
        "# and your batch_size is 20, then only when you have batches of length 75 \n",
        "# utilising all the memory. \n",
        "\n",
        "# An efficient batching mechanism would change the batch size \n",
        "# depending on the sequence length to make sure around 1500 tokens were being processed each iteration.'''\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))  #can change this .src to a more generic extension, #TODO: Review\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heehw6CvrhvJ"
      },
      "source": [
        "# How to get masks and count the tokens in source and target sentences. Note that \n",
        "\n",
        "def masks_tokens(src_token_ids_batch, trg_token_ids_batch, pad_token_id, device):\n",
        "    \n",
        "    def masks_tokens_src(src_token_ids_batch, pad_token_id):\n",
        "        batch_size = src_token_ids_batch.shape[0]\n",
        "\n",
        "        # src_mask shape = (B, 1, 1, S) #TODO: check this\n",
        "        # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
        "        src_mask = (src_token_ids_batch != pad_token_id).view(batch_size, 1, 1, -1)\n",
        "        num_src_tokens = torch.sum(src_mask.long())\n",
        "\n",
        "        return src_mask, num_src_tokens\n",
        "\n",
        "\n",
        "\n",
        "    def masks_tokens_trg(trg_token_ids_batch, pad_token_id):\n",
        "        batch_size = trg_token_ids_batch.shape[0]\n",
        "        device = trg_token_ids_batch.device\n",
        "\n",
        "        # Same as src_mask but we additionally want to mask future tokens (want to predict)\n",
        "        # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
        "        sequence_length = trg_token_ids_batch.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
        "        trg_padding_mask = (trg_token_ids_batch != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
        "        trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
        "\n",
        "        # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
        "        trg_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
        "        num_trg_tokens = torch.sum(trg_padding_mask.long())\n",
        "\n",
        "        return trg_mask, num_trg_tokens\n",
        "\n",
        "    src_mask, num_src_tokens = masks_tokens_src(src_token_ids_batch, pad_token_id)\n",
        "    trg_mask, num_trg_tokens = masks_tokens_trg(trg_token_ids_batch, pad_token_id)\n",
        "\n",
        "    return src_mask, trg_mask, num_src_tokens, num_trg_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cCZ4Wjbe-L4"
      },
      "source": [
        "# Training Loop: Ignore\n",
        "for batch_idx, token_ids_batch in enumerate(token_ids_loader):\n",
        "            src_token_ids_batch, trg_token_ids_batch_input, trg_token_ids_batch_gt = get_src_and_trg_batches(token_ids_batch)\n",
        "            src_mask, trg_mask, num_src_tokens, num_trg_tokens = get_masks_and_count_tokens(src_token_ids_batch, trg_token_ids_batch_input, pad_token_id, device)\n",
        "\n",
        "            # log because the KL loss expects log probabilities (just an implementation detail)\n",
        "            predicted_log_distributions = baseline_transformer(src_token_ids_batch, trg_token_ids_batch_input, src_mask, trg_mask)\n",
        "            smooth_target_distributions = label_smoothing(trg_token_ids_batch_gt)  # these are regular probabilities\n",
        "\n",
        "            if is_train:\n",
        "                custom_lr_optimizer.zero_grad()  # clean the trainable weights gradients in the computational graph\n",
        "\n",
        "            loss = kl_div_loss(predicted_log_distributions, smooth_target_distributions)\n",
        "\n",
        "            if is_train:\n",
        "                loss.backward()  # compute the gradients for every trainable weight in the computational graph\n",
        "                custom_lr_optimizer.step()  # apply the gradients to weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TXNlRP8rhvJ"
      },
      "source": [
        "# View some of our samples\n",
        "def custom_dataloader(data_dir, batch_size, device):\n",
        "    train_dataset, val_dataset, src_field_processor, trg_field_processor = get_language_datasets(data_dir=DATA_PATH)\n",
        "    \n",
        "    '''batch_size_fn: \n",
        "    Function of three arguments (new example to add, current count of examples in the batch, and current effective batch size)\n",
        "            that returns the new effective batch size resulting from adding\n",
        "            that example to a batch. This is useful for dynamic batching, where\n",
        "            this function would add to the current effective batch size the\n",
        "            number of tokens in the new example.'''\n",
        "\n",
        "    # using default sorting function which\n",
        "    train_token_ids_loader, val_token_ids_loader = BucketIterator.splits(\n",
        "     datasets=(train_dataset, val_dataset), batch_size=batch_size,\n",
        "     device=device,\n",
        "     sort_within_batch=True,\n",
        "     batch_size_fn=batch_size_fn\n",
        "    )\n",
        "\n",
        "    return train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnjeSwCerhvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5842086c-5250-4bbb-8317-40dc974b57cd"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r-1UPtxrhvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32ef7fe-c664-4317-b56e-10d55b75be90"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "train_token_ids_loader, val_token_ids_loader, src_field_processor, trg_field_processor = custom_dataloader(data_dir=DATA_PATH, \n",
        "                                                                                                           batch_size=8, device=device)\n",
        "\n",
        "# These are generator objects\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading en-fr.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "en-fr.tgz: 100%|██████████| 26.3M/26.3M [00:07<00:00, 3.59MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data/iwslt/en-fr/IWSLT16.TED.tst2010.en-fr.fr.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2011.en-fr.en.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2013.en-fr.fr.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2012.en-fr.en.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2011.en-fr.fr.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2013.en-fr.en.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2014.en-fr.fr.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2012.en-fr.fr.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.dev2010.en-fr.fr.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2010.en-fr.en.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.tst2014.en-fr.en.xml\n",
            "data/iwslt/en-fr/IWSLT16.TED.dev2010.en-fr.en.xml\n",
            "data/iwslt/en-fr/train.tags.en-fr.fr\n",
            "data/iwslt/en-fr/train.tags.en-fr.en\n",
            "It took 73.89170169830322 seconds to prepare the data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2bGpBD9rhvJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ZgyrLMrhvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "a67217a3-0bf6-4045-8bff-540222026bc4"
      },
      "source": [
        "pad_token_id = src_field_processor.vocab.stoi[PAD_TOKEN]\n",
        "print(pad_token_id)\n",
        "\n",
        "for batch in train_token_ids_loader:\n",
        "    # Visually inspect that masks make sense\n",
        "    src_padding_mask, trg_mask, num_src_tokens, num_trg_tokens = masks_tokens(batch.src, batch.trg, pad_token_id, device)\n",
        "    break\n",
        "\n",
        "# Check vocab size\n",
        "print('The source vocabulary length: {}.'.format(len(src_field_processor.vocab)))\n",
        "print('The target vocabulary length: {}.'.format(len(trg_field_processor.vocab)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-18457d0f7753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_token_ids_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Visually inspect that masks make sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_src_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trg_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mpostprocessing\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             max_len = self.fix_length + (\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqd6CA08rhvK"
      },
      "source": [
        "# helper function to inspect the text\n",
        "def sample_text_from_loader(src_field_processor, trg_field_processor, token_ids_loader, num_samples=2, sample_src=True, sample_trg=True, show_padded=False):\n",
        "    \n",
        "    assert sample_src or sample_trg, f'Either src or trg or both must be enabled.'\n",
        "\n",
        "    for b_idx, token_ids_batch in enumerate(token_ids_loader):\n",
        "        if b_idx == num_samples:  # Number of sentence samples to print\n",
        "            break\n",
        "\n",
        "        print('-' * 10)\n",
        "        if sample_src:\n",
        "            print(\"Source text:\", end=\"\\t\")\n",
        "            for token_id in token_ids_batch.src[0]:  # print only the first example from the batch\n",
        "                src_token = src_field_processor.vocab.itos[token_id]\n",
        "\n",
        "                if src_token == PAD_TOKEN and not show_padded:\n",
        "                    continue\n",
        "\n",
        "                print(src_token, end=\" \")\n",
        "            print()\n",
        "\n",
        "        if sample_trg:\n",
        "            print(\"Target text:\", end=\"\\t\")\n",
        "            for token_id in token_ids_batch.trg[0]:\n",
        "                trg_token = trg_field_processor.vocab.itos[token_id]\n",
        "\n",
        "                if trg_token == PAD_TOKEN and not show_padded:\n",
        "                    continue\n",
        "\n",
        "                print(trg_token, end=\" \")\n",
        "            print(\"\\n\")\n",
        "\n",
        "sample_text_from_loader(src_field_processor, trg_field_processor, train_token_ids_loader, num_samples=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ8QiOpQrhvK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKMaNMdBrhvK"
      },
      "source": [
        "#jachiam\n",
        "\n",
        "\n",
        "B = 5\n",
        "L = 32\n",
        "d = 16\n",
        "w = 8\n",
        "\n",
        "x = torch.as_tensor(np.random.rand(B,L,d), dtype=torch.float32)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in = 16, d_proj = 8, n_heads = 2, causal = True):  #ask josh why causal here  \n",
        "        #a causal convolution has a window that overlaps only the current and previous timesteps\n",
        "        #Causal convolutions are necessary because it would be cheating if the CNN was able to \n",
        "        #“see” information from the future timesteps that it is trying to predict.\n",
        "        super().__init__()\n",
        "        self.d_proj = d_proj\n",
        "        self.n_heads = n_heads\n",
        "        self.qkv_lin = nn.Linear(d_in, 3 * d_proj * n_heads, bias=False)\n",
        "        self.causal = True\n",
        "\n",
        "    def _mask(self, qk):\n",
        "        mask = 1e9 * (torch.tril(torch.ones_like(qk)) - 1)\n",
        "        return torch.tril(qk) + mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv_lin = self.qkv_lin(x).reshape(*x.shape[:2], self.n_heads, self.d_proj * 3) # B x L x n x 3d\n",
        "        qkv_lin = qkv_lin.permute(0,2,1,3)                  # B x n x L x 3d\n",
        "        q, k, v = torch.split(qkv_lin, self.d_proj, dim=-1) # B x n x L x d\n",
        "        q_pre = q.reshape(-1, *q.shape[2:])                 # Bn x L x d\n",
        "        k_pre = k.reshape(-1, *k.shape[2:]).permute(0,2,1)  # Bn x d x L\n",
        "        v_pre = v.reshape(-1, *v.shape[2:])                 # Bn x L x d\n",
        "        qk = torch.bmm(q_pre,k_pre)                         # Bn x L x L\n",
        "        qk_ = self._mask(qk) if self.causal else qk\n",
        "        softmax_qk = torch.softmax(qk_, axis=-1)\n",
        "        \n",
        "        y = torch.bmm(softmax_qk, v_pre)                    # Bn x L x d\n",
        "        y = y.reshape(x.shape[0], self.n_heads, x.shape[1], self.d_proj)  # B x n x L x d\n",
        "        y = y.permute(0,2,1,3)\n",
        "        y = y.reshape(*x.shape[:2],-1)\n",
        "        return y, softmax_qk.reshape(x.shape[0], self.n_heads, x.shape[1], x.shape[1])\n",
        "        #q, k, v = torch.split(qkv_lin, self.d_proj, dim=2)\n",
        "        #return q, k, v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRCskIKnrhvK"
      },
      "source": [
        "embedding = nn.Embedding(10, 3) # an Embedding module containing 10 tensors of size 3\n",
        "embedding\n",
        "\n",
        "input = torch.LongTensor([[1,2,4],[4,3,2], [2,3,1]])\n",
        "input2 = torch.LongTensor([[0,8], [7,6]])\n",
        "print(input)\n",
        "print(embedding(input))\n",
        "\n",
        "print(input2)\n",
        "print(embedding(input2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyPGaN4irhvK"
      },
      "source": [
        "# Input : Embeddings + Positional Encodings\n",
        "src_vocab_size = len(src_field_processor.vocab)\n",
        "trg_vocab_size = len(trg_field_processor.vocab)\n",
        "# Embeddings\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
        "        super().__init__()\n",
        "#         self.embeddings_lookup = nn.Embedding(length,  embedding_dim = 512)\n",
        "        self.source_embedding = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim = 512)\n",
        "        self.target_embedding = nn.Embedding(num_embeddings=trg_vocab_size, embedding_dim = 512)\n",
        "        \n",
        "#     def forward(x, token_ids_batch):\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "# Positional Encoding\n",
        "# class PositionalEncoding(nn.Module):\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIgRXr5CrhvK"
      },
      "source": [
        "\n",
        "embedding = nn.Embedding(1000,512)\n",
        "embedding(torch.LongTensor([3,4]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWbztV_prhvK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4ZR2As0rhvK"
      },
      "source": [
        "# Inspect embedding architecture\n",
        "emb = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim = 512)\n",
        "\n",
        "\n",
        "# batch_id, batch = enumerate(train_token_ids_loader)\n",
        "for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
        "    print(b_idx)\n",
        "#     print(token_ids_batch)\n",
        "\n",
        "    if b_idx >= 2: \n",
        "        break\n",
        "        \n",
        "    \n",
        "    if b_idx == 0:\n",
        "        for token_id in token_ids_batch.src[0]:  \n",
        "            print(\"The token\")\n",
        "            print(token_id)\n",
        "            print(token_ids_batch.src[0])\n",
        "    #         print(token_ids_batch.src[b_idx])\n",
        "            print(\"Shape of token: {}\".format((token_ids_batch.src[0].shape)))\n",
        "            print(\"The Embedding\")\n",
        "            print(emb(token_ids_batch.src[0]))   #need CPU device\n",
        "            print(\"Size of Embedding: {}\".format(emb(token_ids_batch.src[0]).shape))\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVLKn7uerhvK"
      },
      "source": [
        "d_model = 512 # model dimension\n",
        "# for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
        "#     if b_idx < 2:\n",
        "#         print(token_ids_batch)\n",
        "#         print(token_ids_batch.src.ndim)\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, dataloader, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embeddings_lookup = nn.Embedding(num_embeddings = vocab_size, embedding_dim = 512)\n",
        "        \n",
        "        \n",
        "    def forward(self, token_ids_batch):\n",
        "        return(self.embeddings_lookup(token_ids_batch)*math.sqrt(512))\n",
        "    \n",
        "\n",
        "wow = InputEmbedding(dataloader = train_token_ids_loader, vocab_size = src_vocab_size)\n",
        "\n",
        "for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
        "    if b_idx >= 1:\n",
        "        break\n",
        "        \n",
        "    if b_idx < 1:\n",
        "        for token_id in token_ids_batch.src[0]:  \n",
        "            print(wow.forward(token_ids_batch.src))\n",
        "            print(wow.forward(token_ids_batch.trg))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X49KnQHCrhvK"
      },
      "source": [
        "# PE(pos, 2i)= sin(pos/(10000)^(2i/d_model))\n",
        "# PE(pos, 2i+1)= cos(pos/(10000)^(2i/d_model))\n",
        "\n",
        "# max possible sequence length here is 100, sampled input and output sequences smaller than 100, but we might want a buffer so set to 150\n",
        "\n",
        "# position_id = torch.arange(0, 150).unsqueeze(1)\n",
        "# print(position_id)\n",
        "\n",
        "# inter = pos/(np.power(10000, (2 * (i//2)) / np.float32(d_model)))\n",
        "\n",
        "# i=0\n",
        "# expo = torch.pow(10000., 2*i/512)\n",
        "# print(expo)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = 512\n",
        "        self.dropout = nn.Dropout(p= 0.1)\n",
        "        \n",
        "        def get_inter(pos, i, d_model):\n",
        "            return pos* (1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)))\n",
        "\n",
        "        self.wave_pos = get_inter(torch.arange(position).unsqueeze(1), torch.arange(d_model).unsqueeze(1), d_model = 512)\n",
        "#         self.wave_pos = get_inter(np.arange(position)[:, np.newaxis],  np.arange(d_model)[np.newaxis, :],  d_model = 512)\n",
        "        \n",
        "        # apply sin to even indices in the array; 2i\n",
        "        wave_pos[:, 0::2] = np.sin(wave_pos[:, 0::2])\n",
        "  \n",
        "        # apply cos to odd indices in the array; 2i+1\n",
        "        wave_pos[:, 1::2] = np.cos(wave_pos[:, 1::2])\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(x, embeddings):\n",
        "        return self.dropout(wave_pos + embeddings)\n",
        "        \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFTZq7frrhvK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "404bGR3lrhvK"
      },
      "source": [
        "tokens = 10\n",
        "dimensions = 64\n",
        "   \n",
        "# PE(pos, 2i)= sin(pos/((10000)^(2i/d_model)))\n",
        "# PE(pos, 2i+1)= cos(pos/((10000)^(2i/d_model)))\n",
        "\n",
        "    \n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(InputEmbeddings, self).__init__()\n",
        "        self.tokens = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tokens(x) * math.sqrt(self.d_model)\n",
        "\n",
        "    \n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1) # position indexes 0-4999\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2) *  -(math.log(10000.0) / d_model)) # 10000 = 2 * max_len\n",
        "        div_term = torch.pow(10000., -torch.arange(0, d_model, 2, dtype=torch.float)/d_model )\n",
        "\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogf2xHlirhvK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD7EOWM7rhvK",
        "outputId": "47b463cd-b224-4c06-d75d-6a20db78cd63"
      },
      "source": [
        "# So we pass the token ids through the embedding and positional encoding (separately or sequentially). I want to do it separately\n",
        "pure_emb = InputEmbeddings( vocab = src_vocab_size, d_model = 512)\n",
        "\n",
        "pos_enc = PositionalEncoding(d_model = 512, dropout=0.1, max_len=5000)\n",
        "d_matrix = 64\n",
        "n_heads = 8\n",
        "d_model = 512\n",
        "\n",
        "\n",
        "\n",
        "lin_layers = nn.Linear(in_features=d_model, out_features= 3 * d_matrix * n_heads, bias=False) #input: Batch_size X n_tokens X d_model\n",
        "\n",
        "\n",
        "print(lin_layers)\n",
        "\n",
        "for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
        "    if b_idx >= 1:\n",
        "        break\n",
        "        \n",
        "        \n",
        "    if b_idx < 1:\n",
        "        first = 0\n",
        "        for token_id in token_ids_batch.src[0]: \n",
        "            if first == 0:\n",
        "                print(\"Source sentence embedding\")\n",
        "                print(pure_emb.forward(token_ids_batch.src))\n",
        "                print(\"Source sentence embedding plus positional encoding and dropout\")\n",
        "                print(pos_enc.forward(pure_emb.forward(token_ids_batch.src)))\n",
        "                print(pos_enc.forward(pure_emb.forward(token_ids_batch.src)).shape)\n",
        "                first += 1\n",
        "\n",
        "            \n",
        "\n",
        "# Sweet\n",
        "# Now, self-attention\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=512, out_features=1536, bias=False)\n",
            "Source sentence embedding\n",
            "tensor([[[ 26.2299, -24.1689,  36.3473,  ..., -19.3473,   8.8194,   0.7304],\n",
            "         [-32.2750,  11.1693, -18.6711,  ..., -47.1638,  -3.1729,  15.1690],\n",
            "         [ 22.2667,  17.1060, -22.6408,  ...,  52.9551,  59.0051,  13.5322],\n",
            "         ...,\n",
            "         [ 36.6176,  52.9275,  -1.8158,  ..., -28.0755,  38.5611,  11.6662],\n",
            "         [ -8.5879,  -6.3386,  -2.1202,  ..., -35.4681,   2.0181,  14.2503],\n",
            "         [-16.4602,   3.1160,  -0.1835,  ...,  -5.7261, -26.7490,  14.4420]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Source sentence embedding plus positional encoding and dropout\n",
            "tensor([[[ 29.1443, -25.7432,  40.3859,  ..., -20.3859,   0.0000,   1.9227],\n",
            "         [ -0.0000,  13.0106, -19.8325,  ..., -51.2932,  -0.0000,  17.9656],\n",
            "         [  0.0000,  18.5443, -24.1160,  ...,  59.9502,  65.5614,  16.1469],\n",
            "         ...,\n",
            "         [  0.0000,  58.5026,  -2.7260,  ...,  -0.0000,   0.0000,  14.0735],\n",
            "         [-10.3765,  -6.3092,  -3.4628,  ..., -38.2979,   2.2444,  16.9448],\n",
            "         [ -0.0000,   4.5608,  -0.7569,  ...,  -5.2513, -29.7189,  17.1578]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc7UMTK3rhvK",
        "outputId": "3d2d1fc4-09d1-488c-a95d-090994ad8da1"
      },
      "source": [
        "def clones(module, n_copies):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n_copies)])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7awY9BX8rhvK"
      },
      "source": [
        "class Attention1(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, dropout):\n",
        "        super(Attention1, self).__init__()\n",
        "        # take the token embedding and multiply by W matrices (512 x (512/number of heads))\n",
        "        assert d_model%n_heads == 0, f'Model dimension must be divisible by the number of heads.'\n",
        "        d_matrix = int(d_model/n_heads)\n",
        "        \n",
        "#         self.init = nn.init.xavier_uniform_(m.weight)\n",
        "        self.heads = n_heads\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.qkv_lin = nn.Linear(in_features=d_model, out_features= 3 * d_matrix * n_heads, bias=False) \n",
        "        #input of qkv linear: Batch_size X L_tokens X d_model\n",
        "        # output of qkv linear: Batch_size x L_tokens x d_model*3\n",
        "           \n",
        "        \n",
        "#         self.linears = clones(nn.Linear(in_features=d_model, out_features=d_model), n_copies=4)     \n",
        "#         W_Q = torch.rand(d_model, d_matrix)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        return(self.qkv_lin(x))\n",
        "        \n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnJ1GMJprhvK",
        "outputId": "9d2b5e05-88ad-420e-d9ba-6ccec43d5f1a"
      },
      "source": [
        "# So we pass the token ids through the embedding and positional encoding (separately or sequentially). I want to do it separately\n",
        "pure_emb = InputEmbeddings( vocab = src_vocab_size, d_model = 512)\n",
        "\n",
        "pos_enc = PositionalEncoding(d_model = 512, dropout=0.1, max_len=5000)\n",
        "d_matrix = 64\n",
        "n_heads = 8\n",
        "d_model = 512\n",
        "\n",
        "\n",
        "\n",
        "att = Attention1(n_heads = 8, d_model = 512, dropout=0.1)\n",
        "\n",
        "print(lin_layers)\n",
        "\n",
        "for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
        "    if b_idx >= 1:\n",
        "        break\n",
        "        \n",
        "        \n",
        "    if b_idx < 1:\n",
        "        first = 0\n",
        "        for token_id in token_ids_batch.src[0]: \n",
        "            if first == 0:\n",
        "                print(\"Source sentence embedding\")\n",
        "                EMB = pure_emb.forward(token_ids_batch.src)\n",
        "                print(EMB)\n",
        "                print(\"Source sentence embedding plus positional encoding and dropout\")\n",
        "                POS = pos_enc.forward(EMB)\n",
        "                print(POS)\n",
        "                print(POS.shape)\n",
        "                print(\"Linear Layer Passthrough\")\n",
        "                QKV_LIN = lin_layers.forward(POS)\n",
        "#                 print(QKV_LIN)\n",
        "                print(QKV_LIN.shape)\n",
        "                print(\"Attempt at reshaping, so we pop the 3*d_model into a different dimension\")\n",
        "#                 print(QKV_LIN.shape[:2], n_heads, d_matrix)\n",
        "#                 print(QKV_LIN.reshape(*QKV_LIN.shape[:2], n_heads, d_matrix*3))\n",
        "                print(QKV_LIN.reshape(*QKV_LIN.shape[:2], n_heads, d_matrix*3).shape)\n",
        "    \n",
        "                QKV_RESHAPED = QKV_LIN.reshape(QKV_LIN.shape[0], QKV_LIN.shape[1], n_heads, d_matrix*3)\n",
        "#                 print(QKV_RESHAPED.shape)\n",
        "                print(\"First we change the shape in this way\")\n",
        "                QKV_PERMUTED = QKV_LIN.reshape(QKV_LIN.shape[0], n_heads, QKV_LIN.shape[1], d_matrix*3)\n",
        "                print(QKV_PERMUTED.shape)\n",
        "                \n",
        "                print(\"So now we have the reshaped tensor, we have our heads, each with QKV joined like [QKV_1, QKV_2 ... , QKV_n_heads]\")\n",
        "                print(\"Now we want to split the matrices into W_Q, W_K, W_V, in that order\")\n",
        "                Q, K, V = torch.split(QKV_PERMUTED, split_size_or_sections = d_matrix, dim=-1) # B x n x L x d\n",
        "                print(Q.shape)\n",
        "#                 q_pre = Q.reshape(-1, Q.shape[0], Q.shape[1]) # equivalent to Q.reshape(Q.shape[2]*Q.shape[3], Q.shape[0], Q.shape[1])\n",
        "#  q.reshape(-1, *q.shape[2:]) \n",
        "                \n",
        "                q_pre2 = Q.reshape(Q.shape[0]*Q.shape[1], Q.shape[2], Q.shape[3])\n",
        "                k_pre2 = K.reshape(K.shape[0]*K.shape[1], K.shape[3], K.shape[2]) # note that the last 2 dimensions are transposed here for multiplication\n",
        "                v_pre2 = V.reshape(V.shape[0]*V.shape[1], V.shape[2], V.shape[3])\n",
        "                \n",
        "                print(\"Shape before mult\")\n",
        "#                 print(q_pre.shape)\n",
        "                print(q_pre2.shape)\n",
        "                print(k_pre2.shape)\n",
        "#                 print((q_pre2*k_pre2).shape)\n",
        "#                 print(torch.bmm(q_pre2, k_pre2).shape)\n",
        "#                 print(torch.matmul(q_pre2, k_pre2).shape)\n",
        "        \n",
        "                print(\"Now QK_soft\")\n",
        "                QK = torch.matmul(q_pre2, k_pre2)/math.sqrt(d_matrix)\n",
        "                QK_soft = F.softmax(QK, dim = -1)  # dim : A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\n",
        "\n",
        "\n",
        "                print(QK_soft.shape)\n",
        "                print(\"Final values\")\n",
        "                \n",
        "                final_val = torch.matmul(QK_soft,v_pre2)\n",
        "        \n",
        "#                 y = y.reshape(x.shape[0], self.n_heads, x.shape[1], self.d_proj)\n",
        "                print(final_val.shape)\n",
        "                y = final_val.reshape(POS.shape[0], n_heads, POS.shape[1], d_matrix)\n",
        "                y = y.permute(0,2,1,3)   # B x L x n x d\n",
        "                y = y.reshape(*POS.shape[:2],-1)\n",
        "                print(\"reshaped final\")\n",
        "#                 print(y)\n",
        "                \n",
        "                x = final_val.reshape(POS.shape[0], POS.shape[1], n_heads, d_matrix)\n",
        "                x = x.reshape(POS.shape[0], POS.shape[1], -1)\n",
        "                print(\"new test\")\n",
        "#                 print(x)\n",
        "             \n",
        "                first += 1\n",
        "\n",
        "            \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=512, out_features=1536, bias=False)\n",
            "Source sentence embedding\n",
            "tensor([[[ 2.6072e+01, -1.8959e+01, -2.1574e+01,  ..., -1.0317e+01,\n",
            "          -2.5158e+01, -2.2811e+01],\n",
            "         [ 1.8705e+01,  9.5811e+00,  2.2174e+01,  ...,  1.4913e+00,\n",
            "           4.0880e-02,  1.4107e+01],\n",
            "         [-2.0561e+01,  1.7480e+00,  1.7252e+01,  ...,  2.5553e+00,\n",
            "          -1.9070e+01, -8.2803e+00],\n",
            "         ...,\n",
            "         [-4.2242e+00,  2.3923e+00,  4.3365e+01,  ..., -9.9742e+00,\n",
            "           3.2115e+01,  4.0188e+01],\n",
            "         [-2.7822e+01,  2.6559e+01,  3.8341e+01,  ...,  3.9539e+01,\n",
            "          -5.4620e+00,  1.7267e+01],\n",
            "         [ 3.1210e+01,  1.0616e+01, -4.8651e-01,  ...,  2.7768e+01,\n",
            "           2.5556e+00,  5.9681e+00]]], grad_fn=<MulBackward0>)\n",
            "Source sentence embedding plus positional encoding and dropout\n",
            "tensor([[[ 2.8969e+01, -1.9954e+01, -2.3972e+01,  ..., -1.0352e+01,\n",
            "          -2.7953e+01, -2.4234e+01],\n",
            "         [ 0.0000e+00,  1.1246e+01,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           4.5538e-02,  1.6785e+01],\n",
            "         [-0.0000e+00,  0.0000e+00,  2.0209e+01,  ...,  3.9503e+00,\n",
            "          -2.1188e+01, -8.0892e+00],\n",
            "         ...,\n",
            "         [-5.4309e+00,  1.8270e+00,  4.8511e+01,  ..., -9.9714e+00,\n",
            "           3.5687e+01,  4.5765e+01],\n",
            "         [-3.2011e+01,  2.9681e+01,  4.1915e+01,  ...,  4.5043e+01,\n",
            "          -6.0655e+00,  2.0297e+01],\n",
            "         [ 3.4229e+01,  0.0000e+00, -1.6497e+00,  ...,  3.1965e+01,\n",
            "           2.8431e+00,  0.0000e+00]]], grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 32, 512])\n",
            "Linear Layer Passthrough\n",
            "torch.Size([1, 32, 1536])\n",
            "Attempt at reshaping, so we pop the 3*d_model into a different dimension\n",
            "torch.Size([1, 32, 8, 192])\n",
            "First we change the shape in this way\n",
            "torch.Size([1, 8, 32, 192])\n",
            "So now we have the reshaped tensor, we have our heads, each with QKV joined like [QKV_1, QKV_2 ... , QKV_n_heads]\n",
            "Now we want to split the matrices into W_Q, W_K, W_V, in that order\n",
            "torch.Size([1, 8, 32, 64])\n",
            "Shape before mult\n",
            "torch.Size([8, 32, 64])\n",
            "torch.Size([8, 64, 32])\n",
            "Now QK_soft\n",
            "torch.Size([8, 32, 32])\n",
            "Final values\n",
            "torch.Size([8, 32, 64])\n",
            "reshaped final\n",
            "new test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBVyNh3PrhvL"
      },
      "source": [
        "\n",
        "# Now, self-attention\n",
        "\n",
        "class Attention2(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, dropout):\n",
        "        super(Attention2, self).__init__()\n",
        "        # take the token embedding and multiply by W matrices (512 x (512/number of heads))\n",
        "        assert d_model%n_heads == 0, 'Model dimension must be divisible by the number of heads.'\n",
        "        d_matrix = int(d_model/n_heads)\n",
        "        self.d_matrix = d_matrix\n",
        "        \n",
        "        # self.init = nn.init.xavier_uniform_(m.weight)\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.qkv_lin = nn.Linear(in_features=d_model, out_features= 3 * self.d_matrix * self.n_heads, bias=False) \n",
        "        \n",
        "        #input of qkv linear: Batch_size X n_tokens X d_model\n",
        "        # output of qkv linear: Batch_size x n_tokens x d_model*3\n",
        "        \n",
        "    def forward(self, x):\n",
        "        QKV_LIN = self.qkv_lin(x).reshape(*x.shape[:2], self.n_heads, self.d_matrix*3)\n",
        "#         QKV_LIN = self.qkv_lin(x).reshape(x.shape[0], self.n_heads, x.shape[1], self.d_matrix * 3)\n",
        "        \n",
        "        QKV_PERMUTED = QKV_LIN.reshape(QKV_LIN.shape[0], self.n_heads, QKV_LIN.shape[1], self.d_matrix*3)\n",
        "        Q, K, V = torch.split(QKV_PERMUTED, split_size_or_sections = self.d_matrix, dim=-1)\n",
        "        \n",
        "        q_prep = Q.reshape(Q.shape[0]*Q.shape[1], Q.shape[2], Q.shape[3])\n",
        "        k_prep = K.reshape(K.shape[0]*K.shape[1], K.shape[3], K.shape[2])\n",
        "        v_prep = V.reshape(V.shape[0]*V.shape[1], V.shape[2], V.shape[3])\n",
        "        \n",
        "        QK = torch.matmul(q_prep, k_prep)/math.sqrt(self.d_matrix)\n",
        "        QK_soft = self.dropout(F.softmax(QK, dim = -1))  # dim : A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\n",
        "        # Masks and Dropout\n",
        "        QKV = torch.matmul(QK_soft,v_prep)\n",
        "        \n",
        "#         out = QKV.reshape(x.shape[0], self.n_heads, x.shape[1], self.d_matrix)\n",
        "        out = QKV.reshape(x.shape[0], x.shape[1], self.n_heads, self.d_matrix)\n",
        "        out = out.reshape(x.shape[0], x.shape[1], -1)\n",
        "        \n",
        "        return out, QK_soft.reshape(x.shape[0], self.n_heads, x.shape[1], x.shape[1])\n",
        "        \n",
        "    \n",
        "#      1 x 8 x 23 x 192\n",
        "#      1 x 8 x 23 x 64\n",
        "#          8 x 23 x 64\n",
        "#          8 x 64 x 23\n",
        "#       n_heads x L x L\n",
        "#       n_heads x L x 64\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reenKYalrhvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJR5mSyPrhvL"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, d_in, d_within, dropout=0.1):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.linear_in = nn.Linear(d_in, d_within)\n",
        "        self.linear_out = nn.Linear(d_within, d_in)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear_out(self.dropout(F.relu(self.linear_in(x))))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH6Z9CLirhvL",
        "outputId": "108e7ca0-46b0-471c-9bf0-4bdece57a3a6"
      },
      "source": [
        "pure_emb = InputEmbeddings( vocab = src_vocab_size, d_model = 512)\n",
        "\n",
        "pos_enc = PositionalEncoding(d_model = 512, dropout=0.1, max_len=5000)\n",
        "d_matrix = 64\n",
        "n_heads = 8\n",
        "d_model = 512\n",
        "\n",
        "att = Attention2(n_heads = 8, d_model = 512, dropout=0.1)\n",
        "ffnn = FFNN(d_in = d_model, d_within=2048)\n",
        "\n",
        "print(lin_layers)\n",
        "\n",
        "for b_idx, token_ids_batch in enumerate(train_token_ids_loader):\n",
        "    if b_idx >= 1:\n",
        "        break\n",
        "        \n",
        "    if b_idx < 1:\n",
        "        first = 0\n",
        "        for token_id in token_ids_batch.src[0]: \n",
        "            if first == 0:\n",
        "                print(\"Source sentence embedding\")\n",
        "                EMB = pure_emb.forward(token_ids_batch.src)\n",
        "                print(EMB)\n",
        "                print(\"Source sentence embedding plus positional encoding and dropout\")\n",
        "                POS = pos_enc.forward(EMB)\n",
        "                print(POS)\n",
        "                print(POS.shape)\n",
        "#                 print(POS.size)\n",
        "                print(\"Linear Layer Passthrough\")\n",
        "                QKV_attention, QK_soft = att.forward(POS)\n",
        "                print(QKV_attention.shape)\n",
        "                print(QKV_attention)\n",
        "                QKV_FF = ffnn(QKV_attention)\n",
        "                print(\"After feed forward\")\n",
        "                print(QKV_FF)\n",
        "                print(QKV_FF.shape)\n",
        "                \n",
        "            \n",
        "                first += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=512, out_features=1536, bias=False)\n",
            "Source sentence embedding\n",
            "tensor([[[-23.3949, -31.2965,   2.6015,  ...,   8.6640,  18.8246, -30.0368],\n",
            "         [ -9.3731, -33.2603,  29.3171,  ..., -27.7103,  16.1654,  20.7341],\n",
            "         [-15.8117,  23.4906,  -8.3615,  ..., -16.2120, -36.1879, -31.1977],\n",
            "         [  8.6436,   9.3361, -16.0813,  ..., -21.0107, -17.1292, -12.6608]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Source sentence embedding plus positional encoding and dropout\n",
            "tensor([[[-25.9943, -33.6627,   2.8905,  ...,  10.7377,  20.9162, -32.2631],\n",
            "         [ -9.4796, -36.3556,  33.4877,  ..., -29.6781,  17.9616,  24.1490],\n",
            "         [-16.5582,  25.6382,  -8.2501,  ..., -16.9022, -40.2086, -33.5530],\n",
            "         [  9.7609,   9.2735, -17.5957,  ...,  -0.0000, -19.0322, -12.9565]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 4, 512])\n",
            "Linear Layer Passthrough\n",
            "torch.Size([1, 4, 512])\n",
            "tensor([[[-3.2665e+00,  1.4062e+01,  8.0322e-01,  ..., -6.2547e-28,\n",
            "          -2.4050e-27, -5.0491e-28],\n",
            "         [-2.6309e+00, -1.3497e+01,  9.8727e+00,  ...,  6.6628e+00,\n",
            "           1.0130e+01,  1.3818e+01],\n",
            "         [-1.6601e+01, -2.4393e+01, -9.4758e+00,  ..., -9.4593e+00,\n",
            "           1.4016e+01,  1.7761e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.2820e+00,\n",
            "           1.3423e+01,  8.6916e+00]]], grad_fn=<ViewBackward>)\n",
            "After feed forward\n",
            "tensor([[[-2.8227,  2.3237, -0.8117,  ...,  0.2776,  3.1071, -8.8610],\n",
            "         [ 5.6521, -0.8084, -0.1922,  ...,  0.4355,  0.6380, -8.9181],\n",
            "         [ 0.3862, -5.1959, -1.8519,  ..., -1.6185, -0.7047,  1.1777],\n",
            "         [-1.4112, -1.2260, -6.9988,  ..., -2.8169, -1.9353, -7.0251]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([1, 4, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWaAY8K8rhvL"
      },
      "source": [
        "# Make the Encoder Layer (Self Attention and FFNN)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, attn, ff, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = attn\n",
        "        self.ff   = ff\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHsGakRWrhvL"
      },
      "source": [
        "#jachiam\n",
        "\n",
        "\n",
        "B = 5\n",
        "L = 32\n",
        "d = 16\n",
        "w = 8\n",
        "\n",
        "x = torch.as_tensor(np.random.rand(B,L,d), dtype=torch.float32)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in = 16, d_proj = 8, n_heads = 2, causal = True):  #ask josh why causal here  \n",
        "        #a causal convolution has a window that overlaps only the current and previous timesteps\n",
        "        #Causal convolutions are necessary because it would be cheating if the CNN was able to \n",
        "        #“see” information from the future timesteps that it is trying to predict.\n",
        "        super().__init__()\n",
        "        self.d_proj = d_proj\n",
        "        self.n_heads = n_heads\n",
        "        self.qkv_lin = nn.Linear(d_in, 3 * d_proj * n_heads, bias=False)\n",
        "        self.causal = True\n",
        "\n",
        "    def _mask(self, qk):\n",
        "        mask = 1e9 * (torch.tril(torch.ones_like(qk)) - 1)\n",
        "        return torch.tril(qk) + mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv_lin = self.qkv_lin(x).reshape(*x.shape[:2], self.n_heads, self.d_proj * 3) # B x L x n x 3d\n",
        "        qkv_lin = qkv_lin.permute(0,2,1,3)                  # B x n x L x 3d \n",
        "        q, k, v = torch.split(qkv_lin, self.d_proj, dim=-1) # B x n x L x d\n",
        "        q_pre = q.reshape(-1, *q.shape[2:])                 # Bn x L x d\n",
        "        k_pre = k.reshape(-1, *k.shape[2:]).permute(0,2,1)  # Bn x d x L\n",
        "        v_pre = v.reshape(-1, *v.shape[2:])                 # Bn x L x d\n",
        "        qk = torch.bmm(q_pre,k_pre)                         # Bn x L x L\n",
        "        qk_ = self._mask(qk) if self.causal else qk\n",
        "        softmax_qk = torch.softmax(qk_, axis=-1)\n",
        "        \n",
        "   \n",
        "        \n",
        "        y = torch.bmm(softmax_qk, v_pre)                    # Bn x L x d\n",
        "        y = y.reshape(x.shape[0], self.n_heads, x.shape[1], self.d_proj)  # B x n x L x d\n",
        "        y = y.permute(0,2,1,3)   # B x L x n x d\n",
        "        y = y.reshape(*x.shape[:2],-1)\n",
        "        \n",
        "        return y, softmax_qk.reshape(x.shape[0], self.n_heads, x.shape[1], x.shape[1])\n",
        "    \n",
        "        #q, k, v = torch.split(qkv_lin, self.d_proj, dim=2)\n",
        "        #return q, k, v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RhaTpdzrhvL",
        "outputId": "49cf06dc-4173-4ce2-84d6-0ac5df715d1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQFTgpBgrhvL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# # PE(pos, 2i)= sin(pos/((10000)^(2i/d_model)))\n",
        "# # PE(pos, 2i+1)= cos(pos/((10000)^(2i/d_model)))\n",
        "\n",
        "# d_model = 512\n",
        "# a = torch.exp(torch.arange(0, d_model, 2) *  -(math.log(10000.0) / d_model))\n",
        "\n",
        "# i=1\n",
        "# aux = torch.zeros(256) + 10000.\n",
        "# # print(aux)\n",
        "\n",
        "# b = torch.pow(10000., -torch.arange(0, d_model, 2, dtype=torch.float)/d_model )\n",
        "\n",
        "# print(a)\n",
        "# print(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNxKETldrhvL"
      },
      "source": [
        "# Dummy data\n",
        "src_vocab_size = 20\n",
        "trg_vocab_size = 20\n",
        "src_token_ids_batch = torch.randint(1, 10, size=(3, 2))\n",
        "trg_token_ids_batch = torch.randint(1, 10, size=(3, 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3fcFkhxrhvL"
      },
      "source": [
        "# dropout_rate = 0.1\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "# Multiheaded attention\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewP_u3aerhvL"
      },
      "source": [
        "# Encoder Implementation (In the AIAYN paper, we will stack 6 of these, may experiment with more)\n",
        "# In each encoder layer, there is an attention mechanism and there a FFNN\n",
        "\n",
        "\n",
        "\n",
        "# 1. Multihead Attention = Z\n",
        "\n",
        "# 2. LayerNorm(Z + Drop(Z)) = Y\n",
        "\n",
        "# 3. LayerNorm(Y + Drop(Fat-Relu(Y)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCnI8ELtrhvL"
      },
      "source": [
        "# Decoder Implementation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuFAQglxrhvL"
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is3Pmmq4rhvL"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "    \n",
        "    \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "    \n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtBylR_6rhvL"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvT7iwumrhvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxSOacxPrhvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg9s0QccrhvL"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j7JTfTjrhvM"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgL6uLYErhvM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxItichSrhvM"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = Attention2(n_heads=h, d_model=d_model, dropout=dropout)\n",
        "    ff = FFNN(d_in=d_model, d_within=d_ff, dropout=dropout)\n",
        "#     position = PositionalEncoding(d_model, dropout)\n",
        "    position = PositionalEncoding(d_model = 512, dropout=0.1, max_len=5000)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code.  # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhDJ9VXzrhvM",
        "outputId": "163d07e6-122b-45c1-984d-3ddd0b2cba06"
      },
      "source": [
        "tmp_model = make_model(10, 10, 2)\n",
        "None\n",
        "\n",
        "#TODO: change Embeddings to InputEmbeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-336-5e1617b601b6>:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_sjWcRirhvM",
        "outputId": "25efe062-a2b0-4ac2-8247-10916531ac2c"
      },
      "source": [
        " model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-336-5e1617b601b6>:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHz-NkYrhvM"
      },
      "source": [
        "x = torch.tensor([1, 2, 3, 4])\n",
        "a= torch.unsqueeze(x, 0)\n",
        "b= torch.unsqueeze(x, 1)\n",
        "# c= torch.unsqueeze(x, 2)\n",
        "d= torch.unsqueeze(x, -1)\n",
        "\n",
        "print(x)\n",
        "print(a)\n",
        "print(b)\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sruwaQyjrhvM"
      },
      "source": [
        "\n",
        "# def get_inter(pos, i, d_model):\n",
        "#     x = (1 / np.power(10000., (2 * (i//2)) / np.float32(d_model)))\n",
        "#     return pos*x\n",
        "\n",
        "# def pos_encode(position, d_model):\n",
        "#     self.wave_pos = get_inter(torch.arange(position).unsqueeze(1), torch.arange(d_model).unsqueeze(1), d_model = 512)\n",
        "        \n",
        "#     # apply sin to even indices in the array; 2i\n",
        "#     wave_pos[:, 0::2] = np.sin(wave_pos[:, 0::2])\n",
        "    \n",
        "#     # apply cos to odd indices in the array; 2i+1\n",
        "#     wave_pos[:, 1::2] = np.cos(wave_pos[:, 1::2])\n",
        "        \n",
        "        \n",
        "# def get_angles(pos, i, d_model):\n",
        "#     angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "#     return pos * angle_rates\n",
        "\n",
        "# def positional_encoding(position, d_model):\n",
        "#     angle_rads = get_angles(np.arange(position)[:, np.newaxis],     np.arange(d_model)[np.newaxis, :],                 d_model)\n",
        "  \n",
        "#   # apply sin to even indices in the array; 2i\n",
        "#     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "#   # apply cos to odd indices in the array; 2i+1\n",
        "#     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "#     pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "#     return pos_encoding\n",
        "\n",
        "# pos_encoding = positional_encoding(tokens, dimensions)\n",
        "# pos_enc = pos_encode(tokens, dimensions)\n",
        "# print (pos_encoding.shape)\n",
        "# # print(pos_encoding)\n",
        "\n",
        "# print(np.arange(10))\n",
        "# print(np.arange(10)[:, np.newaxis])\n",
        "\n",
        "# print(torch.arange(10))\n",
        "# print(torch.arange(10).unsqueeze(1))\n",
        "\n",
        "# # print(np.arange())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}